{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "\n",
    "The tf-idf vectors rely on counts of words in documents. A different kind of vector, a word vector, looks at the context of words. These word vectors are able to discover interesting things about words, such as synomyms, or words in similar categories or contexts. \n",
    "\n",
    "A word vector is a numeric vector that represents the meaning of a word in context. Tom Mikolov developed the idea of word vectors by training a neural network to predict words occurring near a target word. He called the system word2vec. Since this is an unsupervised approach, large, freely available text can be used for training. \n",
    "\n",
    "There are two approaches to training word vectors:\n",
    "\n",
    "1. skip-gram: predict the context (output) from an input word\n",
    "2. CBOW (continous bag of words): predict the target word (output) from nearby words (input)\n",
    "\n",
    "Pretrained word vectors are available from GloVe, fastText, and other packages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p. 203 shows how to train your own vectors in gensim\n",
    "# and discuss GloVe and fastText"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
