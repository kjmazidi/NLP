{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "This notebook code is taken from Francois Challot's book *Deep Learning with Python*, published by Manning, and available [on Amazon](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_fkmr0_1?keywords=deep+learning+python+challot&qid=1573571371&sr=8-1-fkmr0). \n",
    "\n",
    "This notebook uses the IMDB movie data set that is built-in with Keras to demonstrate adding an Embedding layer in a model. The code follows a similar path to the RNN and CNN imbd notebooks, but with the extra embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = datasets.imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded as in the previous Keras idmb notebook. The previous notebook used vectorized one-hot representations of the presence/absence of words in an example. This notebook will show a simple approach to representing the first 20 words of each review in an embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2576      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 82,593\n",
      "Trainable params: 82,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 0s 758us/step - loss: 0.6326 - acc: 0.6400 - val_loss: 0.5345 - val_acc: 0.7248\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 0s 585us/step - loss: 0.4634 - acc: 0.7800 - val_loss: 0.4996 - val_acc: 0.7484\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 0s 576us/step - loss: 0.3995 - acc: 0.8207 - val_loss: 0.5067 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 584us/step - loss: 0.3506 - acc: 0.8471 - val_loss: 0.5325 - val_acc: 0.7432\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 587us/step - loss: 0.3047 - acc: 0.8745 - val_loss: 0.5653 - val_acc: 0.7338\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 572us/step - loss: 0.2642 - acc: 0.8946 - val_loss: 0.6041 - val_acc: 0.7298\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 597us/step - loss: 0.2292 - acc: 0.9111 - val_loss: 0.6529 - val_acc: 0.7262\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 576us/step - loss: 0.1988 - acc: 0.9245 - val_loss: 0.6992 - val_acc: 0.7178\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 576us/step - loss: 0.1712 - acc: 0.9367 - val_loss: 0.7608 - val_acc: 0.7190\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 590us/step - loss: 0.1485 - acc: 0.9460 - val_loss: 0.8208 - val_acc: 0.7096\n"
     ]
    }
   ],
   "source": [
    "# set up the Embedding layer in a Sequential model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 8, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy reached to about 95% in this approach. This approach is still a bag of words approach in that each word is considered in isolation. More powerful approaches can capture relationships between words. This can be done with a 1D convolutional layer or recurrent layers. Both of these approaches are demonstrated in other notebooks.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "The one-hot vectors are sparse binary high-dimensional vectors. A lower-dimensional approach is to learn word embeddings from the data. Word embeddings learn vector representations based on word co-occurrence. Words that tend to occur together probably are related in some way, so their vectors should be similar. \n",
    "\n",
    "\n",
    "Word embeddings can be learned at the same time as training occurs. This was attempted in the example above, with limited results because our data was small and the model not complex enough. \n",
    "\n",
    "Another way to use embeddings is to use pretrained embeddings from other sources. Ideally, the best approach would be to either train embeddings or use pretrained embeddings in the domain of the problem. Fortunately, the pretrained embeddings tend to be general enough for many appplications. Popular pretrained embeddings include Word2vec and GloVe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
