{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Learning about ACL 2020 from analysis of the paper abstracts\n",
        "\n",
        "This notebook used a data set that I scraped from [ACL 2020 accepted paper](https://www.aclweb.org/anthology/events/acl-2020/#2020-acl-main) abstracts.\n",
        "\n",
        "Could I use NLP techniques to get the big picture of what the conference was about?\n",
        "\n",
        "I tried 3 techniques:\n",
        "* topic modeling\n",
        "* TF-IDF keyword extraction\n",
        "* multi-word extraction with NLTK collocations()\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the data\n",
        "\n",
        "with open('abs.csv', 'r') as f:\n",
        "    docs = f.read().lower().splitlines()\n",
        "    \n",
        "len(docs)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "1224"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Modeling\n",
        "\n",
        "I'll use Gensim for the topic modeling. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models, corpora\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TOPICS = 10"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess docs\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(docs, stopwords):\n",
        "    \"\"\"\n",
        "    Tokenize, remove stopwords and non-alpha tokens.\n",
        "    param: docs - a list of raw text documents\n",
        "    return: a list of processed tokens\n",
        "    \"\"\"\n",
        "    \n",
        "    processed_docs = []\n",
        "    for doc in docs:\n",
        "        tokens = [wnl.lemmatize(t) for t in word_tokenize(doc.lower()) if t.isalpha()]\n",
        "        tokens = [t for t in tokens if t not in stopwords]\n",
        "        processed_docs.append(tokens)\n",
        "        \n",
        "    return processed_docs"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_list = stopwords.words('english')\n",
        "stopword_list += ['propose', 'approach', 'paper', 'show', 'result', 'system', 'also', 'two', 'different']\n",
        "preprocessed_docs = preprocess(docs, stopword_list)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# look at one processed doc\n",
        "print(\" \".join(preprocessed_docs[5])) # join the list of words"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversation achieved remarkable research attention recently however generating informative response multiple relevant knowledge without losing fluency coherence still one main challenge address issue proposes method us recurrent knowledge interaction among response decoding step incorporate appropriate knowledge furthermore introduce knowledge copy mechanism using pointer network copy word external knowledge according knowledge attention distribution joint neural conversation model integrates recurrent knowledge copy kic performs well generating informative response experiment demonstrate model fewer parameter yield significant improvement competitive baseline datasets average bleu ab duconv average bleu ab knowledge format textual amp amp structured language english amp amp chinese\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# the dictionary maps words to id numbers\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# represent the doc tokens in numeric form\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in preprocessed_docs]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# build an LDA model\n",
        "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(NUM_TOPICS):\n",
        "    top_words = [t[0] for t in lda_model.show_topic(i, 9)]\n",
        "    print(\"\\nTopic\", str(i), ':', top_words)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 0 : ['language', 'model', 'task', 'translation', 'method', 'data', 'sentence', 'using', 'corpus']\n",
            "\n",
            "Topic 1 : ['model', 'task', 'language', 'method', 'sentence', 'training', 'performance', 'work', 'representation']\n",
            "\n",
            "Topic 2 : ['model', 'language', 'word', 'task', 'method', 'data', 'learning', 'work', 'sentence']\n",
            "\n",
            "Topic 3 : ['model', 'language', 'task', 'performance', 'data', 'set', 'method', 'training', 'neural']\n",
            "\n",
            "Topic 4 : ['model', 'language', 'task', 'text', 'word', 'method', 'data', 'performance', 'learning']\n",
            "\n",
            "Topic 5 : ['model', 'neural', 'translation', 'task', 'sentence', 'information', 'language', 'data', 'performance']\n",
            "\n",
            "Topic 6 : ['model', 'task', 'method', 'language', 'neural', 'information', 'representation', 'new', 'framework']\n",
            "\n",
            "Topic 7 : ['model', 'language', 'task', 'data', 'method', 'information', 'text', 'framework', 'generation']\n",
            "\n",
            "Topic 8 : ['model', 'task', 'method', 'text', 'data', 'performance', 'word', 'learning', 'using']\n",
            "\n",
            "Topic 9 : ['model', 'language', 'data', 'task', 'translation', 'using', 'ha', 'graph', 'method']\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Modeling Results\n",
        "\n",
        "There is a lot of overlapping words in the topics. The words *model* and *language* occurred in all topics even when I rediced the number of topics to 4. Finding the optimal number of topics is one of the most difficult aspects of topic modeling. [This paper](https://www.researchgate.net/publication/283947121_A_heuristic_approach_to_determine_an_appropriate_number_of_topics_in_topic_modeling) discusses an approach using the perplexity metric to find an optimal number of topics.\n",
        "\n",
        "I would say that the results here are disappointing because the topics are not well separated. In part I can blame the data. It's obvious from looking at the topics that most papers were about language models trained on a corpus and many involve the machine translation task. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "Next I try to extract key words using TF-IDF in sklearn."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "docs2 = [\" \".join(doc) for doc in preprocessed_docs]"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create word counts for the docs \n",
        "count_vectorizer =CountVectorizer() \n",
        " \n",
        "word_counts=count_vectorizer.fit_transform(docs2)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# use sklearn's tfidf transformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True) \n",
        "tfidf_transformer.fit(word_counts)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# extract idf values \n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"]) \n",
        " \n",
        "# sort ascending \n",
        "results = df_idf.sort_values(by=['idf_weights'])\n",
        "\n",
        "print(results.head(n=20))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             idf_weights\n",
            "model           1.347967\n",
            "task            1.666565\n",
            "language        1.813587\n",
            "method          2.060963\n",
            "data            2.099429\n",
            "performance     2.101883\n",
            "neural          2.205334\n",
            "work            2.293585\n",
            "training        2.336145\n",
            "using           2.351794\n",
            "experiment      2.377355\n",
            "text            2.403586\n",
            "ha              2.420337\n",
            "however         2.444269\n",
            "learning        2.461722\n",
            "present         2.465249\n",
            "information     2.497568\n",
            "new             2.501224\n",
            "based           2.569433\n",
            "word            2.573362\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that *model* and *language* have low idf weights because they occur in most of the documents. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# count matrix \n",
        "count_vectors = count_vectorizer.transform(docs) \n",
        " \n",
        "# tf-idf scores \n",
        "tf_idf_vectors = tfidf_transformer.transform(count_vectors)"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for the first doc, find the important words\n",
        "\n",
        "feature_names = count_vectorizer.get_feature_names() \n",
        " \n",
        "#get tfidf vector for first document \n",
        "first_document_vector=tf_idf_vectors[0] \n",
        " \n",
        "#print the scores \n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
        "results = df.sort_values(by=[\"tfidf\"], ascending=False)\n",
        "print(results[:20])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 tfidf\n",
            "directed      0.529406\n",
            "speech        0.524567\n",
            "child         0.290064\n",
            "adult         0.232672\n",
            "trained       0.152596\n",
            "comparable    0.137928\n",
            "acoustically  0.116336\n",
            "linguistic    0.112919\n",
            "phonemic      0.109977\n",
            "acoustic      0.101965\n",
            "differs       0.101965\n",
            "eventually    0.099106\n",
            "prosodic      0.099106\n",
            "acquisition   0.096688\n",
            "repetition    0.094594\n",
            "see           0.092746\n",
            "looking       0.089599\n",
            "partially     0.089599\n",
            "explores      0.085817\n",
            "least         0.085817\n"
          ]
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Results\n",
        "\n",
        "Looking at high-scoring TF-IDF words in a sample document above, these words seems like good 'topic' words for this one document. \n",
        "\n",
        "A weakness for both topic modeling and tf-idf is that they look at single words. The following code sections use NLTK to find multi-word expressions.\n",
        "\n",
        "### NLTK collocations\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(\" \".join(docs2))\n",
        "text_obj = nltk.Text(tokens)"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "text_obj.collocations()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language; machine translation; neural network; language\n",
            "processing; question answering; named entity; shared task; state art;\n",
            "reading comprehension; social medium; publicly available; training\n",
            "data; neural machine; available http; extensive experiment; word\n",
            "embeddings; entity recognition; sentiment analysis; language model;\n",
            "attention mechanism\n"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a good sense of what people are talking about in the abstracts. \n",
        "\n",
        "The lesson here is that in highly technical text with multi-word expressions, approaches that deal with individual tokens may not get optimal results. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nteract": {
      "version": "0.27.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}