{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kjmazidi/NLP/blob/master/Part_4-Documents/Chapter_12_Corpora/Web_Scraping_Example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code accompanies *Natural Language Processing* by KJG Mazidi, all rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Example\n",
    "\n",
    "This notebook provides another example of web scraping, using a [blog](https://karenmazidi.blogspot.com/), which I abandoned long ago. I now blog here, just FYI: [https://www.mazidi-ed.com/](https://www.mazidi-ed.com/)\n",
    "\n",
    "The old blog used Blogger's template, which controls how the blogs are stored on the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, requests is used to access the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://karenmazidi.blogspot.com/'\n",
    "\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then BeautifulSoup is used to create a soup object from the web page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we extract the text, it is quite messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nNever Stop Learning Computer Science\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = soup.get_text()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting paragraphs as shown below doesn't get the blog posts, just the blurb in the heading. The first code block shows that selecting 'p' also gets the html. The second block below shows how to just get the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>\n",
      "Learning about all things computer science, especially machine learning, natural language processing and computer architecture. \n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "for p in soup.select('p'):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning about all things computer science, especially machine learning, natural language processing and computer architecture. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get just the text\n",
    "\n",
    "for p in soup.select('p'):\n",
    "    print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from div\n",
    "\n",
    "We will have to examine the page source to figure out where the text is:\n",
    "\n",
    "* right-click in the page, and View Source \n",
    "* if in Chrome, you can right-click and choose Inspect\n",
    "\n",
    "While *View Source* opens a separate tab, the *Inspect* option opens a side-by-side view. I tend to use the View Source option.\n",
    "\n",
    "Taking a look at the page soure shows that the text of the blog posts is in a div container with class 'snippet-item r-snippetized'. The next code block shows how to extract all of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.findAll('div', {'snippet-item r-snippetized'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have those, we can get the text from each post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google is watching you, but you knew that already. Several years ago I saw a pop-up like this:  It scared the heck out of me so I just exited out of Chrome. It popped up again today and I hit 'I want to play' and this popped up:   Wow. I have heard that this is how Google recruits developers. I don't know if that's true but I exited out of this. Maybe next time . . .\n",
      "\n",
      "\n",
      "I was watching an old Twilight Zone last night, filmed in 1962. The one where an elderly couple comes into a gleaming tech company to purchase new, young artificial bodies for themselves. Spoiler alert: the new technology has unforeseen negative consequences. (Isn't that the theme of every sci-fi work?)  While they are touring displays of young, healthy bodies they may inhabit, for a price, a leggy secretary comes in and tells the executive that he has a call on the video phone. Oh how modern. A video phone. Artificial biology. And yet, a woman still has to be the secretary?  Why was it easier for the writers/producers of this show to imagine videophones, brain transference technology, and more, but they could not imagine a shift in gender roles? Shiny new objects can gain entry into our lives easier than new ideas.   It's always easy to look back and see what others did not see due to their cultural blinders. But what are missing in this moment? How will people 50 years from now lo…\n",
      "\n",
      "\n",
      "This article from Quartz came into my phone's news feed this morning. AI conferences are beginning to think about ethics. Finally. Specifically, the premier machine learning conference NIPS requires a broader impact on society statement for presented research. One of the top natural language processing conferences, EMNLP, will now reject papers on ethical grounds. Whether these moves actually keep AI ethical or just push the dirty parts underground remains to be seen. But we all have a voice in keeping each other in check.   When I was a PhD student, our advisor had us sit in on talks by prospective faculty and arranged for the prospect to meet with just the students. I remember one young man boasting of one NLP project after the other, each more horribly intrusive on innocent lives than the last. All of these done in internships with a major computer manufacturer. Finally, when we got to questions, I asked him, \"Can you describe any project you have done which makes the world …\n",
      "\n",
      "\n",
      "The server microprocessor market hovers around $15 billion/year. The market is largely dominated by Intel, although AMD is hoping to bite off 10% this year.   In contrast, ARM historically focused on the mobile device and IoT markets, but now ARM is shouldering its way into the cloud computing market. Case in point is Amazon's Graviton2 custom processors that use 64-bit ARM cores. Eyes are on ARM due to its reputation for power-efficient cores with good performance.   Ampere's Antra Chip  A new entry into the market this year, Ampere's Altra chip, may be a game-changer. The Altra features up to 80 single-threaded cores utilizing ARM v8 chips.  Ampere's Renee James, a former president of Intel, founded Ampere Computing in 2017 and is currently its Chairman and CEO. Ampere markets the Altra as 'the world's first cloud native processor'. Ampere's Mt. Jade rack dual-socket rack server includes fast DIMM  and NVMe solid-state drive support. The Altra is a chip to …\n",
      "\n",
      "\n",
      "The DFW R User's Group has a monthly Meetup meeting.  On Saturday September 22, 2018, I gave an overview of machine learning with R based on my book.\n",
      "Here is a link to the presentation:  If you just see html, click the \"open with chrome\" link at the top.\n",
      "Here is a link to the quick demo.\n",
      "Download the demo and presentation files here.\n",
      "I had a great time and I highly recommend this group to anyone interested in R.\n",
      "\n",
      "\n",
      "One of the hardest parts of starting out with machine learning used to be finding good data. The UCI Machine Learning Repository was the most comprehensive site, and still is a great resource. The site currently has over 440 data sets. Later,  Kaggle  and similar sites became popular. Google AI is becoming a comprehensive resource for both tools and data. As of this writing, Google has made available over 50 data sets for machine learning, and I'm sure more will come in the future.\n",
      "It's an exciting time to get into machine learning!\n",
      "\n",
      "\n",
      "I just found out about this ongoing project at UTD affiliated with our Robotics and Automation Society. The picture is of UTD's Chess Plaza on the central mall of our beautiful university. The goal of the project is to build robots that can autonomously navigate this chessboard. People can then download an app and play a chess game against the robots.  How cool is that? I'll post again when the project is ready to play.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for post in results:\n",
    "    print(post.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I need to blog more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
