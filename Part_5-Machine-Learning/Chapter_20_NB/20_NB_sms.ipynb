{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kjmazidi/NLP/blob/master/Part_5-Machine-Learning/Chapter_20_NB/20_NB_sms.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code accompanies *Natural Language Processing* by KJG Mazidi, all rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Example\n",
    "\n",
    "Using an SMS Spam data set (slightly modified) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data set is a collection of 5574 SMS messages that have been labeled as ham or spam. The file is a tab-delimited file with the first column the label and the second the message content. The data was edited to remove some unwanted columns and add headings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows and columns: (4837, 2)\n",
      "   spam                                               text\n",
      "0     0  Go until jurong point, crazy.. Available only ...\n",
      "1     0                      Ok lar... Joking wif u oni...\n",
      "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3     0  U dun say so early hor... U c already then say...\n",
      "4     0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# run this block if you are on a local computer\n",
    "\n",
    "df = pd.read_csv('../data/sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "print('rows and columns:', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab only\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab only\n",
    "\n",
    "import io\n",
    "df = pd.read_csv(io.BytesIO(uploaded['sms-spam.csv']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text preprocessing\n",
    "\n",
    "Before applying a machine learning algorithm, the text will be preprocessed by removing stop words and creating a tf-idf representation of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up X and y\n",
    "X = df.text\n",
    "y = df.spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "5    1\n",
       "6    0\n",
       "7    0\n",
       "8    1\n",
       "9    1\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at y\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and test sets\n",
    "\n",
    "Split the data into train and test sets, with 20% of the data going to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3869,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data\n",
    "X_test = vectorizer.transform(X_test)        # transform only the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (3869, 7810)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "test size: (968, 7810)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# take a peek at the data\n",
    "# this is a very sparse matrix because most of the 8613 words don't occur in each sms message\n",
    "\n",
    "print('train size:', X_train.shape)\n",
    "print(X_train.toarray()[:5])\n",
    "\n",
    "print('\\ntest size:', X_test.shape)\n",
    "print(X_test.toarray()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the naive bayes classifier\n",
    "\n",
    "For this data, let's try the MultinomialNB. In a Multinomial Naive Bayes classifier, features are discrete. This fits perfectly for word counts, but can also be used for tfidf representations. \n",
    "\n",
    "We used the default settings. You should always research the documentation and see what these mean:\n",
    "\n",
    "- alpha: additive (Laplace) smoothing (0 for no smoothing)\n",
    "- fit_prior: if True, learn priors from data; if false, use a uniform prior\n",
    "- class_prior: lets you specify class priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.13388472473507365 log of prior: -2.01077611244103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.0107761124410306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# priors\n",
    "import math\n",
    "prior_p = sum(y_train == 1)/len(y_train)\n",
    "print('prior spam:', prior_p, 'log of prior:', math.log(prior_p))\n",
    "\n",
    "# the model prior matches the prior calculated above\n",
    "naive_bayes.class_log_prior_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.643029  , -9.67373923, -9.47714135, ..., -9.53897898,\n",
       "        -9.68907421, -6.31041976],\n",
       "       [-8.23356461, -7.60523447, -9.19154209, ..., -9.19154209,\n",
       "        -8.98794189, -9.19154209]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what else did it learn from the data?\n",
    "# the log likelihood of words given the class\n",
    "\n",
    "naive_bayes.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[848   0]\n",
      " [ 32  88]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# make predictions on the test data\n",
    "pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# print confusion matrix\n",
    "print(confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix has this form\n",
    "#     tp   fp\n",
    "#     fn   tn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9669421487603306\n",
      "\n",
      "precision score (not spam):  0.9636363636363636\n",
      "precision score (spam):  1.0\n",
      "\n",
      "recall score: (not spam) 1.0\n",
      "recall score: (spam) 0.7333333333333333\n",
      "\n",
      "f1 score:  0.846153846153846\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "      \n",
    "print('\\nprecision score (not spam): ', precision_score(y_test, pred, pos_label=0))\n",
    "print('precision score (spam): ', precision_score(y_test, pred))\n",
    "\n",
    "print('\\nrecall score: (not spam)', recall_score(y_test, pred, pos_label=0))\n",
    "print('recall score: (spam)', recall_score(y_test, pred))\n",
    "      \n",
    "print('\\nf1 score: ', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       848\n",
      "           1       1.00      0.73      0.85       120\n",
      "\n",
      "    accuracy                           0.97       968\n",
      "   macro avg       0.98      0.87      0.91       968\n",
      "weighted avg       0.97      0.97      0.96       968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our accuracy?\n",
    "\n",
    "In the data set, there are 4199 not-spam messages out of 4837. The test data distribution is similar. So if we guess not spam every time we would have 87% accuracy. It seems that Naive Bayes did learn something. The accuracy was several points above this simple baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam size in test data: 848\n",
      "test size:  968\n",
      "0.8760330578512396\n"
     ]
    }
   ],
   "source": [
    "print('spam size in test data:',y_test[y_test==0].shape[0])\n",
    "print('test size: ', len(y_test))\n",
    "baseline = y_test[y_test==0].shape[0] / y_test.shape[0] \n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine some wrong classificataions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4179    1\n",
       "677     1\n",
       "2073    1\n",
       "2466    1\n",
       "4721    1\n",
       "3144    1\n",
       "1788    1\n",
       "801     1\n",
       "511     1\n",
       "4062    1\n",
       "4731    1\n",
       "1150    1\n",
       "2754    1\n",
       "924     1\n",
       "444     1\n",
       "1583    1\n",
       "3230    1\n",
       "4757    1\n",
       "165     1\n",
       "4214    1\n",
       "1266    1\n",
       "851     1\n",
       "827     1\n",
       "2932    1\n",
       "3003    1\n",
       "4635    1\n",
       "4363    1\n",
       "1558    1\n",
       "366     1\n",
       "3528    1\n",
       "4479    1\n",
       "2584    1\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test != pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam                                                    1\n",
      "text    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
      "Name: 1536, dtype: object\n",
      "spam                                                    1\n",
      "text    Santa Calling! Would your little ones like a c...\n",
      "Name: 4692, dtype: object\n",
      "spam                                                    1\n",
      "text    You have 1 new voicemail. Please call 08719181...\n",
      "Name: 2915, dtype: object\n",
      "spam                                                    1\n",
      "text    INTERFLORA - ÂIt's not too late to order Inte...\n",
      "Name: 2464, dtype: object\n",
      "spam                                                    1\n",
      "text    CLAIRE here am havin borin time & am now alone...\n",
      "Name: 1101, dtype: object\n",
      "spam                                                    1\n",
      "text    500 free text msgs. Just text ok to 80488 and ...\n",
      "Name: 1268, dtype: object\n",
      "spam                                                    1\n",
      "text    Will u meet ur dream partner soon? Is ur caree...\n",
      "Name: 227, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in [1536, 4692, 2915, 2464, 1101, 1268, 227]:\n",
    "    print(df.loc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "There are capital letters and exclamation points in these messages that were misclassified as not spam, but they really are spam.  The way we preprocessed got rid of this information so our algorithm could not learn from it. \n",
    "\n",
    "Will we get better performance if we process the data differently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Second Try\n",
    "\n",
    "Let's preprocess the text differently to recognize punctuation and caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam                                                    1\n",
      "text     caps   num  &  caps   caps   caps   caps   ca...\n",
      "Name: 1536, dtype: object\n",
      "spam                                                    1\n",
      "text    Santa Calling! Would your little ones like a c...\n",
      "Name: 4692, dtype: object\n",
      "spam                                               1\n",
      "text    You have 1 new voicemail. Please call  num .\n",
      "Name: 2915, dtype: object\n",
      "spam                                                    1\n",
      "text     caps  - ÂIt's not too late to order Interflo...\n",
      "Name: 2464, dtype: object\n",
      "spam                                                    1\n",
      "text     caps  here am havin borin time & am now alone...\n",
      "Name: 1101, dtype: object\n",
      "spam                                                    1\n",
      "text     num  free text msgs. Just text ok to  num  an...\n",
      "Name: 1268, dtype: object\n",
      "spam                                                    1\n",
      "text    Will u meet ur dream partner soon? Is ur caree...\n",
      "Name: 227, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df['text'].replace('[\\d][\\d]+', ' num ', regex=True, inplace=True)\n",
    "df['text'].replace('[!@#*][!@#*]+', ' punct ', regex=True, inplace=True)\n",
    "df['text'].replace('[A-Z][A-Z]+', ' caps ', regex=True, inplace=True)\n",
    "    \n",
    "# these are known problem messages \n",
    "for i in [1536, 4692, 2915, 2464, 1101, 1268, 227]:\n",
    "    print(df.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the rest of the processing\n",
    "X = df.text\n",
    "y = df.spam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the algorithm\n",
    "\n",
    "naive_bayes.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.981404958677686\n",
      "precision score:  1.0\n",
      "recall score:  0.85\n",
      "f1 score:  0.9189189189189189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[848,   0],\n",
       "       [ 18, 102]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "pred = naive_bayes.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "confusion_matrix(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we moved 14 observations that were misclassified as not-spam into spam. We got better recall which in turn led to a better f1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Third Try\n",
    "\n",
    "The next code blocks compare the results using the Binomial classifier instead of the Multinomial classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary=True gives binary data instead of counts\n",
    "vectorizer_b = TfidfVectorizer(stop_words=stopwords, binary=True)\n",
    "\n",
    "# set up X and y\n",
    "X = vectorizer_b.fit_transform(df.text)\n",
    "y = df.spam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_bayes2 = BernoulliNB()\n",
    "naive_bayes2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[847,   1],\n",
       "       [ 13, 107]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions on the test data\n",
    "pred = naive_bayes2.predict(X_test)\n",
    "\n",
    "# print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9855371900826446\n",
      "precision score:  0.9907407407407407\n",
      "recall score:  0.8916666666666667\n",
      "f1 score:  0.9385964912280701\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The Binomial classifier performed better than the Multinomial classifier. The Binomial classifier makes a different model of the data, just the presence or absence of words, rather than counts. If the data coming into the Bernoulli classifier is not binary, the classifier will binarize it. This seems to have worked for this data set, probably because the presences or absences of certain words is a strong predictor of spam or not-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
