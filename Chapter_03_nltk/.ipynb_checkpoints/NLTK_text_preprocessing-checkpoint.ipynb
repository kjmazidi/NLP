{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "The Natural Language Toolkit, NLTK, is an open source Python library to perform many NLP functions such as tokenizing, stemming, part of speech taggng, and more. \n",
    "\n",
    "Instructions for installing NLTK can be found [on the NLTK site](https://www.nltk.org/install.html) for Mac/Unix and Windows systems. \n",
    " \n",
    "After NLTK is installed, the next step is to download NLTK resources. This is best done in a console window:\n",
    "\n",
    "```\n",
    "$ python3\n",
    "\n",
    ">>> import nltk\n",
    ">>> nltk.download()\n",
    "\n",
    "```\n",
    "\n",
    "The nltk.download() instruction will pop up a window with a list of items to download. At a minimum, the 'book' line should be selected to download content associated with the NLTK book.\n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "The process of **tokenizing** is breaking text into smaller units. NLTK can be used to divide text into sentences or individual tokens. A **token** is a word, number, or punctuation mark. The following code block imports NLTK and the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82673707bfaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python split() versus NLTK tokenize\n",
    "\n",
    "We can split text into tokens with Python's split() function, as shown below. However the punctuation attaches to text. Generally the punctuation should be it's own token. As you can see below, NLTK tokenizes the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Sam.', 'Sam', 'I', 'am.', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham.']\n",
      "['I', 'am', 'Sam', '.', 'Sam', 'I', 'am', '.', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "tokens = text.split()\n",
    "print(tokens)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation\n",
    "\n",
    "The NLTK sentence tokenizer performs sentence segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Sam.\n",
      "Sam I am.\n",
      "I do not like green eggs and ham.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Smith went to Dr. Jones.', 'Dr. Jones was trained in the U.S.A.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK will not end a sentence on just any '.'\n",
    "sent_tokenize('Mr. Smith went to Dr. Jones. Dr. Jones was trained in the U.S.A.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " 'was',\n",
       " 'trained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'U.S.A',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK keeps '.' with the token for titles and abbreviations\n",
    "word_tokenize('Mr. Smith went to Dr. Jones. Dr. Jones was trained in the U.S.A.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "\n",
    "Raw text is preprocessed for NLP applications. Preprocessing can involve any of the following:\n",
    "\n",
    "* convert text to lower case\n",
    "* remove punctuation\n",
    "* remove numbers of replace with a generic token like NUM\n",
    "* stem words - removing affixes\n",
    "* lemmatize words - convert to lexical form\n",
    "* remove stop words - reduce text to content words\n",
    "\n",
    "Deciding which preprocessing steps are appropriate for a given NLP application is an important decision. Some of the preprocessing actions can be done with Python functions, some with regular expressions, and some with NLTK methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\" I teach at the University of Texas at Dallas. I\n",
    "     started teaching there in 2016. As of 2018, UTD has been designated \n",
    "     as a national research university!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase, remove punctuation and numbers, tokenize\n",
    "\n",
    "Two different approaches are shown below to get rid of punctuation and numbers. The first code block shows how to do this with regular expressions. The second uses a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'teach', 'at', 'the', 'university', 'of', 'texas', 'at', 'dallas', 'i', 'started', 'teaching', 'there', 'in', 'as', 'of', 'utd', 'has', 'been', 'designated', 'as', 'a', 'national', 'research', 'university']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation and numbers with a regular expression\n",
    "import re\n",
    "\n",
    "text = re.sub(r'[.?!,:;()\\-\\n\\d]',' ', raw_text.lower())\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'teach', 'at', 'the', 'university', 'of', 'texas', 'at', 'dallas', 'i', 'started', 'teaching', 'there', 'in', 'as', 'of', 'utd', 'has', 'been', 'designated', 'as', 'a', 'national', 'research', 'university']\n"
     ]
    }
   ],
   "source": [
    "# use a list comprehension\n",
    "\n",
    "tokens = [t.lower() for t in word_tokenize(raw_text) if t.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "Stop words are words that don't carry much content. These are often function words that help bind a sentence together grammatically. The NLTK list of stop words also includes personal pronouns and common prepositions. NLTK has a list of stop words, but you may find it necessary to make application-dependent stop word lists for different projects. \n",
    "\n",
    "Let's look at NLTK's stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens before stopword removal: 25\n",
      "number of tokens after stopword removal: 11\n",
      "['teach', 'university', 'texas', 'dallas', 'started', 'teaching', 'utd', 'designated', 'national', 'research', 'university']\n"
     ]
    }
   ],
   "source": [
    "# get rid of stop words\n",
    "print('number of tokens before stopword removal:', len(tokens))\n",
    "tokens_content = [t for t in tokens if t not in stopwords]\n",
    "print('number of tokens after stopword removal:', len(tokens_content))\n",
    "print(tokens_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatizing\n",
    "\n",
    "Stemming involves removing affixes. The result may be tokens that are not actually words or have different meanings:\n",
    "\n",
    "```\n",
    "university is stemmed to univers\n",
    "```\n",
    "\n",
    "Lemmatization can do some strange things as well:\n",
    "\n",
    "```\n",
    "'as' is lemmatized to 'a'\n",
    "\n",
    "'has' is lemmatized to 'ha'\n",
    "```\n",
    "\n",
    "But in general, using word normalization is helpful to group together words with the same root meaning. For example, \"education educational educationally\" would all be normalized to 'education'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed tokens:\n",
      " ['i', 'teach', 'at', 'the', 'univers', 'of', 'texa', 'at', 'dalla', 'i', 'start', 'teach', 'there', 'in', 'as', 'of', 'utd', 'ha', 'been', 'design', 'as', 'a', 'nation', 'research', 'univers']\n",
      "\n",
      "lemmatized tokens:\n",
      " ['i', 'teach', 'at', 'the', 'university', 'of', 'texas', 'at', 'dallas', 'i', 'started', 'teaching', 'there', 'in', 'a', 'of', 'utd', 'ha', 'been', 'designated', 'a', 'a', 'national', 'research', 'university']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(t) for t in tokens]\n",
    "print('stemmed tokens:\\n', stemmed)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in tokens]\n",
    "print('\\nlemmatized tokens:\\n', lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK in other languages\n",
    "\n",
    "Some NLTK features support other languages, primarily European languages. Other tools we will look at later in the course support international languages such as Arabic and Chinese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola mi amor.', 'Como estas?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "tokenizer.tokenize('Hola mi amor. Como estas?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
