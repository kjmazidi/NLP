{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "This notebook demonstrates how to calculate tf-idf using Python. The data used is extracted from online textbooks written at the high school and early college level. Four documents are used, representing the text of one chapter from each of 4 different topics:\n",
    "\n",
    "* anatomy\n",
    "* business law\n",
    "* economics\n",
    "* geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 13 the pacific and antarctica the immense '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num_docs = 4\n",
    "\n",
    "with open('data/anat.txt', 'r') as f:\n",
    "    doc_anat = f.read().lower()\n",
    "    doc_anat = doc_anat.replace('\\n', ' ')\n",
    "    \n",
    "\n",
    "with open('data/buslaw.txt', 'r') as f:\n",
    "    doc_buslaw = f.read().lower()\n",
    "    doc_buslaw = doc_buslaw.replace('\\n', ' ')\n",
    "    \n",
    "with open('data/econ.txt', 'r') as f:\n",
    "    doc_econ = f.read().lower()\n",
    "    doc_econ = doc_econ.replace('\\n', ' ')\n",
    "    \n",
    "with open('data/geog.txt', 'r') as f:\n",
    "    doc_geog = f.read().lower()\n",
    "    doc_geog = doc_geog.replace('\\n', ' ')\n",
    "    \n",
    "# look at part of a document\n",
    "doc_geog[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf\n",
    "\n",
    "The code below writes a function to calculate the frequency of a term in a document. Using a Counter() object would make the code faster, but the goal here is seeing how the sausage is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 4054\n"
     ]
    }
   ],
   "source": [
    "# create tf dictionaries for each document\n",
    "\n",
    "vocab = set()  # set of words\n",
    "\n",
    "def create_tf_dict(doc):\n",
    "    tf_dict = {}\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [w for w in tokens if w.isalpha() and w not in stopwords]\n",
    "     \n",
    "    # get term frequencies\n",
    "    for t in tokens:\n",
    "        if t in tf_dict:\n",
    "            tf_dict[t] += 1\n",
    "        else:\n",
    "            tf_dict[t] = 1\n",
    "            \n",
    "    # get term frequencies in a more Pythonic way\n",
    "    token_set = set(tokens)\n",
    "    tf_dict = {t:tokens.count(t) for t in token_set}\n",
    "    \n",
    "    # normalize tf by number of tokens\n",
    "    for t in tf_dict.keys():\n",
    "        tf_dict[t] = tf_dict[t] / len(tokens)\n",
    "        \n",
    "    return tf_dict\n",
    "\n",
    "tf_anat = create_tf_dict(doc_anat)\n",
    "tf_buslaw = create_tf_dict(doc_buslaw)\n",
    "tf_econ = create_tf_dict(doc_econ)\n",
    "tf_geog = create_tf_dict(doc_geog)\n",
    "    \n",
    "    \n",
    "# add to vocab\n",
    "vocab = set(tf_anat.keys())\n",
    "vocab = vocab.union(set(tf_buslaw.keys()))\n",
    "vocab = vocab.union(set(tf_econ.keys()))\n",
    "vocab = vocab.union(set(tf_geog.keys()))\n",
    "\n",
    "print(\"number of unique words:\", len(vocab))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf for \"work\" in anat = 0.00046040515653775324\n",
      "tf for \"work\" in buslaw = 0.0027739251040221915\n",
      "tf for \"work\" in econ = 0.0006854009595613434\n",
      "tf for \"work\" in geog = 0.0009285051067780873\n"
     ]
    }
   ],
   "source": [
    "# get tf for 'work' in each doc\n",
    "\n",
    "print('tf for \"work\" in anat =', tf_anat.get('work'))\n",
    "print('tf for \"work\" in buslaw =', tf_buslaw.get('work'))\n",
    "print('tf for \"work\" in econ =', tf_econ.get('work'))\n",
    "print('tf for \"work\" in geog =', tf_geog.get('work'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idf\n",
    "\n",
    "Make an idf frequency dictionary. Adding +1 to denominator to avoid divide by zero. Adding +1 to numerator to avoid negative idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf_dict = {}\n",
    "\n",
    "vocab_by_topic = [tf_anat.keys(), tf_buslaw.keys(), \n",
    "                  tf_econ.keys(), tf_geog.keys()]\n",
    "\n",
    "for term in vocab:\n",
    "    temp = ['x' for voc in vocab_by_topic if term in voc]\n",
    "    idf_dict[term] = math.log((1+num_docs) / (1+len(temp))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf for work: 0.0\n",
      "idf for inflation: 0.9162907318741551\n"
     ]
    }
   ],
   "source": [
    "# look at idf for 'work'\n",
    "# 0 idf because it occurs in all docs\n",
    "print('idf for work:', idf_dict['work'])\n",
    "\n",
    "# look at idf for 'inflation'\n",
    "# high idf because it occurs in 1 of the 4 docs\n",
    "print('idf for inflation:', idf_dict['inflation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "Create a tf-idf dictionary for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(tf, idf):\n",
    "    tf_idf = {}\n",
    "    for t in tf.keys():\n",
    "        tf_idf[t] = tf[t] * idf[t] \n",
    "        \n",
    "    return tf_idf\n",
    "\n",
    "tf_idf_anat = create_tfidf(tf_anat, idf_dict)\n",
    "tf_idf_buslaw = create_tfidf(tf_buslaw, idf_dict)\n",
    "tf_idf_econ = create_tfidf(tf_econ, idf_dict)\n",
    "tf_idf_geog = create_tfidf(tf_geog, idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may', 0.0),\n",
       " ('additional', 0.0),\n",
       " ('fact', 0.0),\n",
       " ('common', 0.0),\n",
       " ('simple', 0.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the lowest tf-idf terms for the anatomy text\n",
    "doc_term_weights = sorted(tf_idf_anat.items(), key=lambda x:x[1])\n",
    "doc_term_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "anatomy:  [('sympathetic', 0.01581993666909798), ('system', 0.014228798452045322), ('autonomic', 0.012234084357435773), ('parasympathetic', 0.010335691957144016), ('receptors', 0.008648232045773564)]\n",
      "\n",
      "business law:  [('party', 0.023129668959930128), ('damages', 0.023129668959930128), ('breach', 0.011946092759524352), ('nonbreaching', 0.011691920573151495), ('remedies', 0.008133509963931473)]\n",
      "\n",
      "economics:  [('inflation', 0.04542725355647514), ('prices', 0.013816584031001654), ('index', 0.009839082567531481), ('price', 0.009103129690141027), ('basket', 0.005861581104061308)]\n",
      "\n",
      "geography:  [('islands', 0.023609162311520708), ('island', 0.012123623889699824), ('antarctica', 0.008295111082426195), ('ozone', 0.007444330458587611), ('pacific', 0.0068062449907086734)]\n"
     ]
    }
   ],
   "source": [
    "# find the highest tf-idf terms for each document\n",
    "doc_term_weights = sorted(tf_idf_anat.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"\\nanatomy: \", doc_term_weights[:5])\n",
    "\n",
    "doc_term_weights = sorted(tf_idf_buslaw.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"\\nbusiness law: \", doc_term_weights[:5])\n",
    "\n",
    "doc_term_weights = sorted(tf_idf_econ.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"\\neconomics: \", doc_term_weights[:5])\n",
    "\n",
    "doc_term_weights = sorted(tf_idf_geog.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"\\ngeography: \", doc_term_weights[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these documents, tf-idf did a good job of identifying important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
