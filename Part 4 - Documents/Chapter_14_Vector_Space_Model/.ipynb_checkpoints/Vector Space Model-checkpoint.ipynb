{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space model\n",
    "\n",
    "A vector space model represents documents as vectors in a shared vector space. First, a vocabulary is created as a set of all words in the corpus. Then the vocabulary is alphabetically sorted into a list. Each document vector is represented as a Python list where each element is the count of the corresponding word in the vocab list. Once vectors are created for each document, cosine similarity can be calculated for pairs of documents. The python library numpy is used for the vector operations.\n",
    "\n",
    "This notebook uses 4 texts on 4 subjects. Each text is divided into halves so that the corpus consists of 8 documents. The first part of the notebook shows how to create a vector space model from scratch in Python. The second part shows how to use vector operations in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each document and break each into 2 pieces\n",
    "# create a list of docs\n",
    "docs = []\n",
    "\n",
    "with open('../school_texts/anat.txt', 'r') as f:\n",
    "    doc_anat = f.read().lower()\n",
    "    doc_anat = doc_anat.replace('\\n', ' ')\n",
    "    i = len(doc_anat)//2\n",
    "    doc_anat1 = doc_anat[:i]\n",
    "    doc_anat2 = doc_anat[i:]\n",
    "    docs.append(word_tokenize(doc_anat1))\n",
    "    docs.append(word_tokenize(doc_anat2))\n",
    "    \n",
    "with open('../school_texts/buslaw.txt', 'r') as f:\n",
    "    doc_buslaw = f.read().lower()\n",
    "    doc_buslaw = doc_buslaw.replace('\\n', ' ')\n",
    "    i = len(doc_buslaw)//2\n",
    "    doc_buslaw1 = doc_buslaw[:i]\n",
    "    doc_buslaw2 = doc_buslaw[i:]\n",
    "    docs.append(word_tokenize(doc_buslaw1))\n",
    "    docs.append(word_tokenize(doc_buslaw2))\n",
    "    \n",
    "with open('../school_texts/econ.txt', 'r') as f:\n",
    "    doc_econ = f.read().lower()\n",
    "    doc_econ = doc_econ.replace('\\n', ' ')\n",
    "    i = len(doc_econ)//2\n",
    "    doc_econ1 = doc_econ[:i]\n",
    "    doc_econ2 = doc_econ[i:]\n",
    "    docs.append(word_tokenize(doc_econ1))\n",
    "    docs.append(word_tokenize(doc_econ2))\n",
    "    \n",
    "with open('../school_texts/geog.txt', 'r') as f:\n",
    "    doc_geog = f.read().lower()\n",
    "    doc_geog = doc_geog.replace('\\n', ' ')\n",
    "    i = len(doc_geog)//2\n",
    "    doc_geog1 = doc_geog[:i]\n",
    "    doc_geog2 = doc_geog[i:]\n",
    "    docs.append(word_tokenize(doc_geog1))\n",
    "    docs.append(word_tokenize(doc_geog2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: remove non-alpha, remove stopwords, lemmatize\n",
    "docs_preprocessed = [[wnl.lemmatize(w) for w in doc if w not in stopwords and w.isalpha()] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 3601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abandoned', 'abdominal', 'abide', 'ability', 'able']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for doc in docs_preprocessed:\n",
    "    doc_set = set(doc)\n",
    "    vocab = vocab.union(doc_set)\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "print('vocab length:', len(vocab)) \n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for doc in docs_preprocessed:\n",
    "    vec = [doc.count(t) for t in vocab]\n",
    "    vectors.append(vec)\n",
    "print(vectors[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute cosine similarity\n",
    "def cos_sim(v1, v2):\n",
    "    return float(dot(v1, v2)) / (norm(v1) * norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity anat1 and vector 1 = 1.00\n",
      "cosine similarity anat1 and vector 2 = 0.72\n",
      "cosine similarity anat1 and vector 3 = 0.05\n",
      "cosine similarity anat1 and vector 4 = 0.05\n",
      "cosine similarity anat1 and vector 5 = 0.06\n",
      "cosine similarity anat1 and vector 6 = 0.06\n",
      "cosine similarity anat1 and vector 7 = 0.06\n",
      "cosine similarity anat1 and vector 8 = 0.10\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity for the first doc, paired with all docs\n",
    "for i, vec in enumerate(vectors):\n",
    "    print('cosine similarity anat1 and vector', i+1, '=', format(cos_sim(vectors[0], vec), '.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In the results above, the cosine similarity of anat1 with itself is 1, the highest similarity. The cosine similarity of anat1 with anat2 is high: 0.72. The cosine similarity with all other docs is low. This approach gave very good results. The preprocessing is key. Without stopword removal, all the docs would seem more similar to each other than they really are. \n",
    "\n",
    "## Using sklearn\n",
    "\n",
    "The sklearn package has vectorizer functions for converting docs to vectors. Sincs the 'docs' above were already tokenized, the code below creates 'docs2' which puts the tokens back together into plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs shape: (8, 3593)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "docs2 = [' '.join(docs_preprocessed[i]) for i in range(len(docs))]\n",
    "\n",
    "tfidf_docs = tfidf_vectorizer.fit_transform(docs2)\n",
    "print('docs shape:', tfidf_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70338879, 0.02614208, 0.0192754 , 0.02797854,\n",
       "        0.02504257, 0.0273924 , 0.05087871]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(tfidf_docs[0], tfidf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We see here that anat1 and anat2 have a high similarity whereas all others are low. The cosine similarity scores are similar to the 'from scratch' code above. The difference is that the from-scratch vectors used term frequency and the sklearn vectors used tf-idf to create the vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
