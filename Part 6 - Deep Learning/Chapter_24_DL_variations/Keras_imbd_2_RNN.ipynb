{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network on the Movie Reviews Data\n",
    "\n",
    "This notebook code is taken from Francois Challot's book *Deep Learning with Python*, published by Manning, and available [on Amazon](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_fkmr0_1?keywords=deep+learning+python+challot&qid=1573571371&sr=8-1-fkmr0). You can see the orignal notebook [on the book's GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb)\n",
    "\n",
    "This notebook tries different RNN models on the data. A previous notebook tried a Dense sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras. preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "# load the data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# pad the data to maxlen\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a Sequential model with Embedding and SimpleRNN layers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Dense, Embedding, SimpleRNN\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1115 12:15:10.415168 4488834496 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1115 12:15:12.175845 4488834496 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 12s 588us/step - loss: 0.6294 - accuracy: 0.6288 - val_loss: 0.4831 - val_accuracy: 0.7708\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 12s 593us/step - loss: 0.3831 - accuracy: 0.8363 - val_loss: 0.3921 - val_accuracy: 0.8262\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 12s 588us/step - loss: 0.2864 - accuracy: 0.8869 - val_loss: 0.3895 - val_accuracy: 0.8456\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 12s 592us/step - loss: 0.2145 - accuracy: 0.9183 - val_loss: 0.3656 - val_accuracy: 0.8558\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 12s 593us/step - loss: 0.1436 - accuracy: 0.9488 - val_loss: 0.4309 - val_accuracy: 0.8284\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 12s 585us/step - loss: 0.0820 - accuracy: 0.9733 - val_loss: 0.4633 - val_accuracy: 0.8530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 12s 586us/step - loss: 0.0468 - accuracy: 0.9862 - val_loss: 0.5249 - val_accuracy: 0.8240\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 12s 593us/step - loss: 0.0267 - accuracy: 0.9930 - val_loss: 0.6087 - val_accuracy: 0.8144\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 12s 584us/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 0.6343 - val_accuracy: 0.8164\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 12s 585us/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.6708 - val_accuracy: 0.8174\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81     12500\n",
      "           1       0.81      0.81      0.81     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.81      0.81      0.81     25000\n",
      "weighted avg       0.81      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(test_data)\n",
    "pred = [1.0 if p>= 0.5 else 0.0 for p in pred]\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleRNN got 81% accuracy, which was lower than the accuracy in the Dense Sequential models in previous notebooks. The SimpleRNN does not perform well on longer sequences such as text. Next, we try the more powerful LSTM. The first block of code below changes only one layer, the SimpleRNN is changed to LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.5088 - accuracy: 0.7527 - val_loss: 0.3812 - val_accuracy: 0.8492\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.2871 - accuracy: 0.8871 - val_loss: 0.2904 - val_accuracy: 0.8814\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.2340 - accuracy: 0.9104 - val_loss: 0.4455 - val_accuracy: 0.8622\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.1997 - accuracy: 0.9270 - val_loss: 0.3032 - val_accuracy: 0.8868\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1749 - accuracy: 0.9355 - val_loss: 0.3043 - val_accuracy: 0.8720\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.1590 - accuracy: 0.9429 - val_loss: 0.3190 - val_accuracy: 0.8632\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.1413 - accuracy: 0.9509 - val_loss: 0.7382 - val_accuracy: 0.7828\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.1324 - accuracy: 0.9543 - val_loss: 0.5358 - val_accuracy: 0.8348\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 26s 1ms/step - loss: 0.1202 - accuracy: 0.9581 - val_loss: 0.4925 - val_accuracy: 0.8502\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 27s 1ms/step - loss: 0.1143 - accuracy: 0.9594 - val_loss: 0.3993 - val_accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86     12500\n",
      "           1       0.89      0.82      0.85     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred = [1.0 if p>= 0.5 else 0.0 for p in pred]\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM performed better than the Simple RNN but still not as good as the Dense Sequential model. For sentiment analysis, the presence or absence of words captured by the Dense Sequential model is more powerful than the LSTM model. However, there are many other NLP machine learning applications where LSTM outperforms a Dense Sequential model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
