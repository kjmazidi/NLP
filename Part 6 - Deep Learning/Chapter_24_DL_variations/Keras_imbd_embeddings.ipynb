{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "This notebook code is taken from Francois Challot's book *Deep Learning with Python*, published by Manning, and available [on Amazon](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_fkmr0_1?keywords=deep+learning+python+challot&qid=1573571371&sr=8-1-fkmr0). You can see the orignal notebook [on the book's GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb)\n",
    "\n",
    "This notebook uses the IMDB movie data set that is built-in with Kerasto demonstrate adding an Embedding layer in a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "max_features = 10000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded as in the previous Keras idmb notebook. The previous notebook used vectorized one-hot representations of the presence/absence of words in an example. This notebook will show a simple approach to representing the first 20 words of each review in an embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "maxlen = 20\n",
    "\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.6638 - acc: 0.6331 - val_loss: 0.6060 - val_acc: 0.7042\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 28us/step - loss: 0.5302 - acc: 0.7551 - val_loss: 0.5180 - val_acc: 0.7324\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.4572 - acc: 0.7887 - val_loss: 0.4961 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 26us/step - loss: 0.4221 - acc: 0.8073 - val_loss: 0.4915 - val_acc: 0.7538\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 26us/step - loss: 0.3992 - acc: 0.8200 - val_loss: 0.4913 - val_acc: 0.7592\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.3800 - acc: 0.8324 - val_loss: 0.4940 - val_acc: 0.7598\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.3633 - acc: 0.8401 - val_loss: 0.4971 - val_acc: 0.7560\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 28us/step - loss: 0.3469 - acc: 0.8508 - val_loss: 0.5022 - val_acc: 0.7584\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.3310 - acc: 0.8592 - val_loss: 0.5085 - val_acc: 0.7540\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 28us/step - loss: 0.3156 - acc: 0.8665 - val_loss: 0.5156 - val_acc: 0.7524\n"
     ]
    }
   ],
   "source": [
    "# set up the Embedding layer in a Sequential model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The validation accuracy reached 75% in this approach. This approach is still a bag of words approach in that each word is considered in isolation. More powerful approaches can capture relationships between words. This can be done with a 1D convolutional layer or recurrent layers. Both of these approaches are demonstrated in other notebooks.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "The one-hot vectors are sparse binary high-dimensional vectors. A lower-dimensional approach is to learn word embeddings from the data. Word embeddings learn vector representations based on word co-occurrence. Words that tend to occur together probably are related in some way, so their vectors should be similar. \n",
    "\n",
    "\n",
    "Word embeddings can be learned at the same time as training occurs. This was attempted in the example above, with limited results because our data was small and the model not complex enough. \n",
    "\n",
    "Another way to use embeddings is to use pretrained embeddings from other sources. Ideally, the best approach would be to either train embeddings or use pretrained embeddings in the domain of the problem. Fortunately, the pretrained embeddings tend to be general enough for many appplications. Popular pretrained embeddings include Word2vec and GloVe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
