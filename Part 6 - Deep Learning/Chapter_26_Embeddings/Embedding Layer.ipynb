{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an embedding layer\n",
    "\n",
    "The notebook uses input data that was processed and picked in notebook 'Embedded Data Pre-Processing'\n",
    "\n",
    "The notebook shows how to add an embedding layer in contrast to using pretrained embeddings as shown in the notebook 'GloVe'\n",
    "\n",
    "This notebook is modified from a [Keras blog post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in preprocessed data. See Embedding Data Pre-Processing notebook for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_samples = pickle.load(open('data/train_samples.pkl', 'rb'))\n",
    "train_labels = pickle.load(open('data/train_labels.pkl', 'rb'))\n",
    "\n",
    "val_samples = pickle.load(open('data/val_samples.pkl', 'rb'))\n",
    "val_labels = pickle.load(open('data/val_labels.pkl', 'rb'))\n",
    "\n",
    "test_samples = pickle.load(open('data/test_samples.pkl', 'rb'))\n",
    "test_labels = pickle.load(open('data/test_labels.pkl', 'rb'))\n",
    "\n",
    "class_names = pickle.load(open('data/class_names.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the vectorizer\n",
    "\n",
    "Use Keras's TextVectorization() function to vectorize the data, using only the top 20K words. Each sample will be truncated or padded to a length of 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word index dictionary in which words map to indices\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3509, 1657, 15, 2, 5562]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "embedding_layer = layers.Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Several layers of Conv1D followed by pooling, ending in a softmax classification layer. Instead of the usual Keras syntax, this example uses syntax from the Functional API: https://www.tensorflow.org/guide/keras/functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560128   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,825,364\n",
      "Trainable params: 2,825,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add more layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize train and validation sets\n",
    "\n",
    "Using vectorizer in this way will right-pad the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Sparse categorical crossentropy is used because the final layer is a multi-class softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: 2.8294 - acc: 0.0897 - val_loss: 2.4797 - val_acc: 0.1408\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 8s 85ms/step - loss: 2.4709 - acc: 0.1388 - val_loss: 2.2748 - val_acc: 0.1883\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: 2.1812 - acc: 0.1999 - val_loss: 1.9760 - val_acc: 0.2706\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 8s 86ms/step - loss: 1.8702 - acc: 0.2879 - val_loss: 1.7445 - val_acc: 0.3571\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: 1.6117 - acc: 0.3820 - val_loss: 1.6463 - val_acc: 0.3943\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 9s 97ms/step - loss: 1.3703 - acc: 0.4865 - val_loss: 1.4617 - val_acc: 0.5099\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 9s 94ms/step - loss: 1.1208 - acc: 0.5873 - val_loss: 1.4270 - val_acc: 0.5446\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 9s 92ms/step - loss: 0.8983 - acc: 0.6804 - val_loss: 1.4769 - val_acc: 0.5511\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.7144 - acc: 0.7492 - val_loss: 1.3435 - val_acc: 0.6234\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 0.5554 - acc: 0.8109 - val_loss: 1.4488 - val_acc: 0.6409\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 8s 90ms/step - loss: 0.4429 - acc: 0.8501 - val_loss: 1.5067 - val_acc: 0.6672\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 9s 92ms/step - loss: 0.3542 - acc: 0.8866 - val_loss: 1.6513 - val_acc: 0.6682\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.2909 - acc: 0.9092 - val_loss: 2.2238 - val_acc: 0.5954\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.2449 - acc: 0.9244 - val_loss: 2.0067 - val_acc: 0.6392\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 9s 95ms/step - loss: 0.2001 - acc: 0.9368 - val_loss: 1.8995 - val_acc: 0.6959\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 9s 100ms/step - loss: 0.1835 - acc: 0.9432 - val_loss: 2.1918 - val_acc: 0.6604\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 9s 95ms/step - loss: 0.1608 - acc: 0.9497 - val_loss: 2.0491 - val_acc: 0.7007\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 9s 95ms/step - loss: 0.1485 - acc: 0.9537 - val_loss: 2.5386 - val_acc: 0.6557\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 0.1278 - acc: 0.9582 - val_loss: 2.2790 - val_acc: 0.6969\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 0.1265 - acc: 0.9600 - val_loss: 2.0908 - val_acc: 0.6927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feaba85c940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model\n",
    "\n",
    "The next code block shows how you could create an end-to-end systems where the input is a text string and the output is the predicted label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
    ")\n",
    "\n",
    "class_names[np.argmax(probabilities[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "pred_labels = [np.argmax(p) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.70      0.65       200\n",
      "           1       0.53      0.49      0.51       202\n",
      "           2       0.58      0.55      0.56       196\n",
      "           3       0.51      0.66      0.58       192\n",
      "           4       0.68      0.78      0.72       196\n",
      "           5       0.86      0.63      0.72       190\n",
      "           6       0.69      0.58      0.63       201\n",
      "           7       0.73      0.73      0.73       200\n",
      "           8       0.74      0.82      0.78       196\n",
      "           9       0.83      0.89      0.86       213\n",
      "          10       0.85      0.89      0.87       188\n",
      "          11       0.80      0.84      0.82       196\n",
      "          12       0.53      0.60      0.56       206\n",
      "          13       0.80      0.64      0.71       190\n",
      "          14       0.81      0.75      0.78       206\n",
      "          15       0.78      0.65      0.71       193\n",
      "          16       0.70      0.70      0.70       223\n",
      "          17       0.91      0.81      0.86       200\n",
      "          18       0.55      0.69      0.61       197\n",
      "          19       0.42      0.37      0.39       215\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.69      0.69      0.69      4000\n",
      "weighted avg       0.69      0.69      0.69      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
