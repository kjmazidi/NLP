{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVe embeddings\n",
    "\n",
    "This notebooks shows how to use GloVe pretrained embeddings. The notebook is modified from a Keras [blog post](https://keras.io/examples/nlp/pretrained_word_embeddings/)\n",
    "\n",
    "Read more about GloVe here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "The notebook uses input data that was processed and picked in notebook 'Embedded Data Pre-Processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in preprocessed data. See Embedding Data Pre-Processing notebook for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_samples = pickle.load(open('data/train_samples.pkl', 'rb'))\n",
    "train_labels = pickle.load(open('data/train_labels.pkl', 'rb'))\n",
    "\n",
    "val_samples = pickle.load(open('data/val_samples.pkl', 'rb'))\n",
    "val_labels = pickle.load(open('data/val_labels.pkl', 'rb'))\n",
    "\n",
    "test_samples = pickle.load(open('data/test_samples.pkl', 'rb'))\n",
    "test_labels = pickle.load(open('data/test_labels.pkl', 'rb'))\n",
    "\n",
    "class_names = pickle.load(open('data/class_names.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the vectorizer\n",
    "\n",
    "Use Keras's TextVectorization() function to vectorize the data, using only the top 20K words. Each sample will be truncated or padded to a length of 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained word embeddings\n",
    "\n",
    "The pretrained GloVe embeddings were downloaded from: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "The file was then expanded in the .keras/datasets folder. \n",
    "\n",
    "The next block of code creates an embeddings index dictionary to map words to the GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B/glove.6B.100d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding matrix, replacing the original token with the GloVe embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17889 words (2111 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Embedding layer, setting trainable to False so that the embeddings are not modified during model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Several layers of Conv1D followed by pooling, ending in a softmax classification layer. Instead of the usual Keras syntax, this example uses syntax from the Functional API: https://www.tensorflow.org/guide/keras/functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize train and validation sets\n",
    "\n",
    "Using vectorizer in this way will right-pad the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Sparse categorical crossentropy is used because the final layer is a multi-class softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 2.7771 - acc: 0.1138 - val_loss: 2.2770 - val_acc: 0.2336\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 5s 57ms/step - loss: 2.1393 - acc: 0.2688 - val_loss: 1.7732 - val_acc: 0.3701\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 5s 57ms/step - loss: 1.6957 - acc: 0.4243 - val_loss: 1.5403 - val_acc: 0.4621\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 1.4203 - acc: 0.5143 - val_loss: 1.3326 - val_acc: 0.5364\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 1.2344 - acc: 0.5818 - val_loss: 1.2879 - val_acc: 0.5581\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 1.0929 - acc: 0.6271 - val_loss: 1.1587 - val_acc: 0.5989\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.9851 - acc: 0.6610 - val_loss: 1.0614 - val_acc: 0.6357\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.8807 - acc: 0.7004 - val_loss: 1.2208 - val_acc: 0.6002\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.7757 - acc: 0.7347 - val_loss: 1.3019 - val_acc: 0.5766\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.6874 - acc: 0.7600 - val_loss: 1.3919 - val_acc: 0.5861\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.6147 - acc: 0.7847 - val_loss: 1.1094 - val_acc: 0.6584\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.5295 - acc: 0.8146 - val_loss: 1.2649 - val_acc: 0.6232\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.4705 - acc: 0.8362 - val_loss: 1.1891 - val_acc: 0.6579\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.4242 - acc: 0.8568 - val_loss: 1.2247 - val_acc: 0.6574\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.3513 - acc: 0.8780 - val_loss: 1.3479 - val_acc: 0.6584\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.3286 - acc: 0.8921 - val_loss: 1.4880 - val_acc: 0.6207\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.2916 - acc: 0.9023 - val_loss: 1.2379 - val_acc: 0.6774\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.2597 - acc: 0.9146 - val_loss: 1.4770 - val_acc: 0.6604\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 6s 61ms/step - loss: 0.2321 - acc: 0.9253 - val_loss: 1.4930 - val_acc: 0.6494\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 6s 61ms/step - loss: 0.2125 - acc: 0.9322 - val_loss: 1.4277 - val_acc: 0.6814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f851667bdf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model\n",
    "\n",
    "The next code block shows how you could create an end-to-end systems where the input is a text string and the output is the predicted label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
    ")\n",
    "\n",
    "class_names[np.argmax(probabilities[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "pred_labels = [np.argmax(p) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.71      0.51       200\n",
      "           1       0.60      0.59      0.60       202\n",
      "           2       0.65      0.59      0.62       196\n",
      "           3       0.44      0.57      0.49       192\n",
      "           4       0.49      0.75      0.60       196\n",
      "           5       0.67      0.62      0.65       190\n",
      "           6       0.78      0.63      0.70       201\n",
      "           7       0.77      0.72      0.75       200\n",
      "           8       0.86      0.82      0.84       196\n",
      "           9       0.94      0.88      0.91       213\n",
      "          10       0.90      0.94      0.92       188\n",
      "          11       0.89      0.78      0.83       196\n",
      "          12       0.62      0.59      0.61       206\n",
      "          13       0.79      0.81      0.80       190\n",
      "          14       0.82      0.84      0.83       206\n",
      "          15       0.70      0.69      0.69       193\n",
      "          16       0.72      0.55      0.62       223\n",
      "          17       0.76      0.82      0.79       200\n",
      "          18       0.52      0.45      0.48       197\n",
      "          19       0.33      0.13      0.19       215\n",
      "\n",
      "    accuracy                           0.67      4000\n",
      "   macro avg       0.68      0.67      0.67      4000\n",
      "weighted avg       0.68      0.67      0.67      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is not terrible for a 20-class classification problem, but not terrific. It is only slightly higher than the other notebook which used an embedding layer instead of pretrained embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
