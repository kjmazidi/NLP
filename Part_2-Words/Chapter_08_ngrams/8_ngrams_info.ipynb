{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams\n",
    "\n",
    "The ngrams() function returns a generator where each item is a tuple containing all the words -- 1 word for unigrams, 2 for bigrams, etc. This looks strange for unigrams so if you wanted to extract the word you would need unigram[0].\n",
    "\n",
    "First we do the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mary',)\n",
      "('had',)\n",
      "('a',)\n",
      "('little',)\n",
      "('lamb',)\n",
      "('.',)\n"
     ]
    }
   ],
   "source": [
    "text = \"Mary had a little lamb .\"\n",
    "tokens = text.split()\n",
    "unigrams = ngrams(tokens, 1)\n",
    "for unigram in unigrams:\n",
    "    print(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mary', 'had')\n",
      "('had', 'a')\n",
      "('a', 'little')\n",
      "('little', 'lamb')\n",
      "('lamb', '.')\n",
      "reconstruct the bigrams\n",
      "Mary had\n",
      "had a\n",
      "a little\n",
      "little lamb\n",
      "lamb .\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(tokens, 2)\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n",
    "\n",
    "# note that if I want to access it again I need to regenerate\n",
    "print(\"reconstruct the bigrams\")\n",
    "bigrams = ngrams(tokens, 2)\n",
    "for bigram in bigrams:\n",
    "    print(bigram[0] + ' ' + bigram[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterators and generators\n",
    "\n",
    "In the examples above we printed the results of the ngrams() method of nltk. These are actually generator objects which brings us to a discussion of Python iterators and generators. \n",
    "\n",
    "## iterators\n",
    "\n",
    "We can view iterators as any Python type that can be used with for loops. Python built-in structures that work with iterators include lists, tuples, dicts and sets. They are iterators because they implement the iterator protocol and the __iter__ method. This method returns an object that has a **next** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want a apple\n",
      "I want a banana\n",
      "I want a kiwi\n"
     ]
    }
   ],
   "source": [
    "for fruit in ['apple', 'banana', 'kiwi']:\n",
    "    print(\"I want a\", fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code worked because Python is sending the **next** item to our variable *fruit*. \n",
    "\n",
    "### generators\n",
    "\n",
    "Python generators are also iterators as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apple', 'banana', 'cucumber')\n",
      "('banana', 'cucumber', 'dill')\n"
     ]
    }
   ],
   "source": [
    "for ngram in ngrams('apple banana cucumber dill'.split(), 3):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing our own generators\n",
    "\n",
    "In the next 2 cells we see a Fibonacci function first done the regular way and second with a generator.\n",
    "\n",
    "What happens in the generator function is that when the generator is called, the body is not executed, rather a generator-iterator object is returned. This object can then be used as an iterator. Python knows it is a generator by use of the **yield** reserved word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fib(max):\n",
    "    numbers = []\n",
    "    a, b = 0, 1\n",
    "    while a < max:\n",
    "        numbers.append(a)\n",
    "        a, b = b, a + b\n",
    "    return numbers\n",
    "\n",
    "fib(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object fib_gen at 0x7f8fb43df200>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fib_gen(max):\n",
    "    a, b = 0, 1\n",
    "    while a < max:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        \n",
    "fib_gen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for item in fib_gen(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this looks really strange, why use generators? This is useful when you have a **huge** amount of data you are processing. Generators will take up less memory by streaming the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle\n",
    "\n",
    "Want if you need to save a Python object to disk to read in later? One way would be to write it out item by item to disk. You may have to do some processing when you read back in a dict for example to identify the key and the value.\n",
    "\n",
    "A better way is to use pickle. The pickle module can serialize and deserialize a Python object. The pickling process converts the object to a byte stream and unpickling does the reverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 5, 'b': 7, 'c': 2}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "my_dict = {'a':5, 'b':7, 'c':2}\n",
    "with open('example.pickle', 'wb') as handle:\n",
    "    pickle.dump(my_dict, handle)\n",
    "    \n",
    "with open('example.pickle', 'rb') as handle:\n",
    "    new_dict = pickle.load(handle)\n",
    "    \n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's look some more at creating bigram and unigram iterator objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died t\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "emma_raw[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.  She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.  Her mother had died t\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_text = emma_raw.replace('\\n', ' ')\n",
    "emma_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty-one',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "emma_tokens = word_tokenize(emma_text)\n",
    "emma_tokens[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create ngrams\n",
    "\n",
    "The ngrams(tokens, n) function takes two arguments. The first is a list of tokens, the second is n for the ngram desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[',)\n",
      "('Emma',)\n",
      "('by',)\n",
      "('Jane',)\n",
      "('Austen',)\n"
     ]
    }
   ],
   "source": [
    "unigrams = ngrams(emma_tokens, 1)\n",
    "count = 1\n",
    "for unigram in unigrams:\n",
    "    print(unigram)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple-dumplings -> 1\n",
      "deposit -> 2\n",
      "lists -> 2\n",
      "agitation -> 16\n",
      "inconstancy -> 2\n"
     ]
    }
   ],
   "source": [
    "unigram_dict = {}\n",
    "for unigram in set(unigrams):\n",
    "    unigram_dict[unigram[0]] = emma_text.count(unigram[0])\n",
    "    \n",
    "count = 1\n",
    "for uni in unigram_dict.keys():\n",
    "    print(uni, '->', unigram_dict[uni])\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', 'Emma')\n",
      "('Emma', 'by')\n",
      "('by', 'Jane')\n",
      "('Jane', 'Austen')\n",
      "('Austen', '1816')\n",
      "('1816', ']')\n",
      "(']', 'VOLUME')\n",
      "('VOLUME', 'I')\n",
      "('I', 'CHAPTER')\n",
      "('CHAPTER', 'I')\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(emma_tokens, 2)\n",
    "count = 1\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", overcome -> 1\n",
      "but correctly -> 1\n",
      "beyond expression -> 1\n",
      "consider the -> 2\n",
      "the wind -> 17\n",
      "CHAPTER XVI -> 9\n",
      "an egg -> 2\n",
      "months later -> 1\n",
      "-- Still -> 2\n",
      "plague you -> 1\n"
     ]
    }
   ],
   "source": [
    "bigram_dict = {}\n",
    "for bigram in set(bigrams):\n",
    "    if bigram not in bigram_dict:\n",
    "        bi = bigram[0] + ' ' + bigram[1]\n",
    "        bigram_dict[bi] = emma_text.count(bi)\n",
    "    \n",
    "count = 1\n",
    "for bi in bigram_dict.keys():\n",
    "    print(bi, '->', bigram_dict[bi])\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, NLTK ngrams() is pretty slow, so in our homework we will build the dictionaries in one program and pickle them. Then unpickle them in the second program.\n",
    "\n",
    "Let's see the most common bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", a -> 2547\n",
      ", an -> 1909\n",
      ", and -> 1880\n",
      "; a -> 918\n",
      "; an -> 878\n",
      "; and -> 867\n",
      "of the -> 672\n",
      "to be -> 649\n",
      ", I -> 570\n",
      "in the -> 505\n",
      "he had -> 499\n",
      "he was -> 486\n",
      "as a -> 474\n",
      ". We -> 441\n",
      "; but -> 427\n",
      ". Weston -> 426\n",
      "and a -> 397\n",
      "I am -> 395\n",
      ", the -> 387\n",
      ". Elton -> 381\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for bi in sorted(bigram_dict, key=bigram_dict.get, reverse=True):\n",
    "    print(bi, '->', bigram_dict[bi])\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
