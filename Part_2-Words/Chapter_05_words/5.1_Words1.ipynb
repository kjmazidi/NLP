{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Words and Counting\n",
    "## Demo for analyzing word counts in a document using Python and WordNet\n",
    "### Code accompanies Section 5.1 Word Counts and Text Analysis\n",
    "\n",
    "This notebook examines some techniques with Python. For sample code, built-in Text documents from nltk are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# imports used in the notebook\n",
    "\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The built-in texts are already tokenized and stored as NLTK Text objects.\n",
    "\n",
    "A minimal amount of preprocessing is done on text4: the inaugural address corpus:\n",
    "* lower case the tokens (text 4 is already tokenized)\n",
    "* count the number of tokens with len()\n",
    "* count the number of unique tokens with (set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of tokens in text4:  149797\n",
      "\n",
      "The number of unique tokens in text4: 9216\n",
      "\n",
      "The first 5 unique tokens in text4: ['!', '\"', '\";', '\"?', '$']\n"
     ]
    }
   ],
   "source": [
    "# lowercase the text\n",
    "tokens4 = [t.lower() for t in text4]\n",
    "\n",
    "print(\"\\nThe number of tokens in text4: \", len(tokens4))\n",
    "set4 = set(tokens4)\n",
    "print(\"\\nThe number of unique tokens in text4:\", len(set4))\n",
    "print(\"\\nThe first 5 unique tokens in text4:\", sorted(set4)[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Preprocessing\n",
    "\n",
    "In the output above, it seems there are a lot of tokens that are punctuation. Let's do more preprocessing:\n",
    "\n",
    "* reduce the tokens to tokens that are alpha and are not stopwords\n",
    "* create the counts again\n",
    "* display the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of important words in text4: 64336\n",
      "\n",
      "The number of unique important words in text4: 8973\n",
      "\n",
      "The first 10 unique important words in text4: ['fellow', 'citizens', 'senate', 'house', 'representatives', 'among', 'vicissitudes', 'incident', 'life', 'event']\n"
     ]
    }
   ],
   "source": [
    "# get rid of punctuation and stopwords\n",
    "tokens4 = [t for t in tokens4 if t.isalpha() and\n",
    "           t not in stopwords.words('english')]\n",
    "print(\"\\nThe number of important words in text4:\", len(tokens4))\n",
    "print(\"\\nThe number of unique important words in text4:\", len(set(tokens4)))\n",
    "print(\"\\nThe first 10 unique important words in text4:\", tokens4[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity\n",
    "\n",
    "There are many varied definitions and formulas for lexical diversity, but they all try to measure how diverse or limited the vocabulary is. Here is one formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexical diversity: 0.14\n"
     ]
    }
   ],
   "source": [
    "# lexical diversity\n",
    "print(\"\\nLexical diversity: %.2f\" % (len(set4) / len(tokens4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas\n",
    "\n",
    "Lemmas are the root form of the word. The next chunk of code reduces the tokens to just lemmas, in order to get a better picture of the kinds of things these documents are 'about'.\n",
    "\n",
    "Future notebooks look in more detail at WordNet, for this notebook we use it's Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of unique lemmas in text4:  7935\n"
     ]
    }
   ],
   "source": [
    "# get the lemmas\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens4]\n",
    "# make unique\n",
    "lemmas_unique = list(set(lemmas))  # ?\n",
    "print(\"\\nThe number of unique lemmas in text4: \", len(lemmas_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of counts\n",
    "\n",
    "How common is each lemma? We can make a dictionary where the key is the lemma and the value is a count of how many times tokens in the documents have that lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citizen 303\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary of counts\n",
    "counts = {t:lemmas.count(t) for t in lemmas_unique}\n",
    "print('citizen', counts['citizen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the least and most common words\n",
    "\n",
    "The book goes into detail about this line: sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "Here are the key points:\n",
    "* sorted() returns a list of tuples: \\[\\('citizen': 303), (...\\)\\] because a dict is not sorted\n",
    "* key=lambda x: x[1] means to sort on the second value of the tuples, which are the counts\n",
    "* reverse=True means sort from high count to low count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words:\n",
      "('government', 651)\n",
      "('people', 623)\n",
      "('nation', 515)\n",
      "('u', 478)\n",
      "('state', 442)\n",
      "\n",
      "5 least common words:\n",
      "('childish', 1)\n",
      "('trim', 1)\n",
      "('unrepealed', 1)\n",
      "('journal', 1)\n",
      "('fifth', 1)\n"
     ]
    }
   ],
   "source": [
    "# print 10 most common words\n",
    "# dicts are unordered so we sort it and put it in a list of tuples\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"5 most common words:\")\n",
    "for i in range(5):\n",
    "    print(sorted_counts[i])\n",
    "\n",
    "print(\"\\n5 least common words:\")\n",
    "for i in range(-1,-6, -1):\n",
    "    print(sorted_counts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP is never perfect but improvements can be made\n",
    "\n",
    "The code above discovered that 'u' was in the 5 most common lemmas. The reality of NLP projects is:\n",
    "\n",
    "* NLP results are not perfect because language is messy.\n",
    "* NLP results are not perfect because the available tools are not perfect.\n",
    "* NLP is perfectable, meaning that results can be incrementally improved with hard work, patience, and persistence.\n",
    "\n",
    "The remaining code blocks in this notebook do some detective work to see what happened with that 'u'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up', 'us'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all words of length 1 or 2 that start with u\n",
    "x = set([t for t in text4 if t.startswith('u') and len(t) < 3])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what happens when 'us' is lemmatized?\n",
    "wnl.lemmatize('us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha. 'Us' is the problem. There are a few possible ways to deal with this:\n",
    "\n",
    "* Use a customized list of stop words, and include 'us'.\n",
    "* Remove words of length 1 from the set of unique lemmas. \n",
    "* Try different lemmatizers to see if better results can be achieved.\n",
    "\n",
    "Adding to the list of stopwords is straighforward:\n",
    "\n",
    "```\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['may', 'must', 'every', 'one']  # add more stop words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
