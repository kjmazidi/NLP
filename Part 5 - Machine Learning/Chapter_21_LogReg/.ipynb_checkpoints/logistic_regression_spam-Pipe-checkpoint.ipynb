{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Pipelines\n",
    "\n",
    "\n",
    "In this notebook we are going to use the sms messages data we used for Naive Bayes, and discuss pipelines a little more. We are also going to use a new metric, log_loss, which is a metric that penalizes wrong classifications. In other words, while we want to see a high accuracy, we want to see a low log_loss. In order to compute log_loss, the predict_proba() function is called to extract probabilities rather than predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports for the next code block\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do logistic regression without pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.984504132231405\n",
      "precision score:  0.926829268292683\n",
      "recall score:  0.95\n",
      "f1 score:  0.9382716049382716\n",
      "log loss:  0.15218610084693873\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "\n",
    "# set up X and y\n",
    "X = df.text\n",
    "y = df.spam\n",
    "\n",
    "# divide into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = TfidfVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data\n",
    "X_test = vectorizer.transform(X_test)        # transform only the test data\n",
    "\n",
    "#train\n",
    "classifier = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "probs = classifier.predict_proba(X_test)\n",
    "print('log loss: ', log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Next we run the algorithm again but use pipelines.\n",
    "\n",
    "So what methods can you put in a pipeline? You can use sklearn class functions or write your own. The intermediate steps in the pipeline must be *transformers*, that is, they need .fit() and .transform() methods, or a fit_transform() method, and the last one in the pipeline  usually is an *estimator*, that is, it implements a .fit() method. \n",
    "\n",
    "In the code block below we created a pipeine named **pipe1** using the Pipeline() method. The arguments to the Pipeline() method are a list of tuples, where the first item in each tuple is an identifier you choose for each stage, and the second item in the tuple is the method for that stage in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# read in data, split raw data into train and test, then use pipeline to transform\n",
    "df = pd.read_csv('../data/sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['spam'], test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('logreg', LogisticRegression(solver='lbfgs', class_weight='balanced')),\n",
    "])\n",
    "\n",
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9834710743801653\n",
      "precision score:  0.9193548387096774\n",
      "recall score:  0.95\n",
      "f1 score:  0.9344262295081968\n",
      "log loss:  0.1520686216817141\n"
     ]
    }
   ],
   "source": [
    "pred = pipe1.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "probs = pipe1.predict_proba(X_test)\n",
    "print('log loss: ', log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The first thing to notice is that there were fewer code steps involved in using the pipeline. The second observation is that the results are slightly different. Let's think about why that might be.\n",
    "\n",
    "Most researchers feel that you should perform transformations like tf-idf on the train and test sets separately so that information about the test data does not leak into the train data. Previously, we did the tf-idf transformation on the entire data, then divided the transformed data into train and test sets. Here, using the pipeline, we did the tf-idf transformation on the train set when we used pipe1.fit() and on the test set later when we did pipe1.predict(). Therefore we should not be surprised that we got different results. \n",
    "\n",
    "Before moving on the cross-validation, let's look at some information we can get from our pipeline. This will help us identify parameters we may want to tune during cross validation.\n",
    "\n",
    "Notice above that sklearn printed out information about the pipeline steps, by name. We can extract that information as well as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tfidf',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                  input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                  min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                  smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                  sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=None, use_idf=True, vocabulary=None)),\n",
       " ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the pipeline steps\n",
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68642287,  0.84627621, -0.02188987, ..., -0.01523863,\n",
       "         0.18401337, -0.99653254]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect coeficients of the model\n",
    "pipe1.named_steps['logreg'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect parameters of the model\n",
    "pipe1.named_steps['logreg'].get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'logreg': LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 1),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'logreg__C': 1.0,\n",
       " 'logreg__class_weight': 'balanced',\n",
       " 'logreg__dual': False,\n",
       " 'logreg__fit_intercept': True,\n",
       " 'logreg__intercept_scaling': 1,\n",
       " 'logreg__l1_ratio': None,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__multi_class': 'warn',\n",
       " 'logreg__n_jobs': None,\n",
       " 'logreg__penalty': 'l2',\n",
       " 'logreg__random_state': None,\n",
       " 'logreg__solver': 'lbfgs',\n",
       " 'logreg__tol': 0.0001,\n",
       " 'logreg__verbose': 0,\n",
       " 'logreg__warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check all the parameters of the pipeline, do this:\n",
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that we can get parameters out of the pipeline, but we can also set them. Below we set a min_df a little higher which will ignore very infrequent terms, and we set the C parameter in logistic regression a little higher. The higher the C term, the more it tamps down the regularization. \n",
    "\n",
    "We see slightly better results by tweaking a couple of parameters. The main point here, though, was to show how the pipeline made it easy to experiment with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9865702479338843\n",
      "log loss:  0.10287741066958897\n"
     ]
    }
   ],
   "source": [
    "# set a couple of parameters\n",
    "pipe1.set_params(tfidf__min_df=3, logreg__C=2.0).fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(\"accuracy: \", accuracy_score(y_test, pred))\n",
    "probs = pipe1.predict_proba(X_test)\n",
    "print(\"log loss: \", log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The syntax of sklearn pipelines takes a little getting used to, but does tend to reduce the number of lines of code you have to read through and as we have seen, makes experimenting a little easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
