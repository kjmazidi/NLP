{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Pipelines\n",
    "\n",
    "\n",
    "In this notebook we are going to use the sms messages data we used for Naive Bayes, and discuss pipelines a little more. We are also going to use a new metric, log_loss, which is a metric that penalizes wrong classifications. In other words, while we want to see a high accuracy, we want to see a low log_loss. In order to compute log_loss, the predict_proba() function is called to extract probabilities rather than predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports for the next code block\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do logistic regression without pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.984504132231405\n",
      "precision score:  0.926829268292683\n",
      "recall score:  0.95\n",
      "f1 score:  0.9382716049382716\n",
      "log loss:  0.15218610084693873\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "\n",
    "# set up X and y\n",
    "X = df.text\n",
    "y = df.spam\n",
    "\n",
    "# divide into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = TfidfVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform the train data\n",
    "X_test = vectorizer.transform(X_test)        # transform only the test data\n",
    "\n",
    "#train\n",
    "classifier = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "probs = classifier.predict_proba(X_test)\n",
    "print('log loss: ', log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Next we run the algorithm again but use pipelines.\n",
    "\n",
    "So what methods can you put in a pipeline? You can use sklearn class functions or write your own. The intermediate steps in the pipeline must be *transformers*, that is, they need .fit() and .transform() methods, or a fit_transform() method, and the last one in the pipeline  usually is an *estimator*, that is, it implements a .fit() method. \n",
    "\n",
    "In the code block below we created a pipeine named **pipe1** using the Pipeline() method. The arguments to the Pipeline() method are a list of tuples, where the first item in each tuple is an identifier you choose for each stage, and the second item in the tuple is the method for that stage in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=True,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# read in data, split raw data into train and test, then use pipeline to transform\n",
    "df = pd.read_csv('../data/sms-spam.csv', header=0, usecols=[1,2], encoding='latin-1')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['spam'], test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(binary=True)),\n",
    "        ('logreg', LogisticRegression(solver='lbfgs', class_weight='balanced')),\n",
    "])\n",
    "\n",
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.984504132231405\n",
      "precision score:  0.926829268292683\n",
      "recall score:  0.95\n",
      "f1 score:  0.9382716049382716\n",
      "log loss:  0.15218610084693873\n"
     ]
    }
   ],
   "source": [
    "pred = pipe1.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred))\n",
    "print('recall score: ', recall_score(y_test, pred))\n",
    "print('f1 score: ', f1_score(y_test, pred))\n",
    "probs = pipe1.predict_proba(X_test)\n",
    "print('log loss: ', log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The first thing to notice is that there were fewer code steps involved in using the pipeline. The second observation is that the results are the same.\n",
    "\n",
    "Before moving on the cross-validation, let's look at some information we can get from our pipeline. This will help us identify parameters we may want to tune during cross validation.\n",
    "\n",
    "Notice above that sklearn printed out information about the pipeline steps, by name. We can extract that information as well as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tfidf',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                  input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                  min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                  smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                  sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=None, use_idf=True, vocabulary=None)),\n",
       " ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the pipeline steps\n",
    "pipe1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68642287,  0.84627621, -0.02188987, ..., -0.01523863,\n",
       "         0.18401337, -0.99653254]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect coeficients of the model\n",
    "pipe1.named_steps['logreg'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect parameters of the model\n",
    "pipe1.named_steps['logreg'].get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('logreg', LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'logreg': LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 1),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'logreg__C': 1.0,\n",
       " 'logreg__class_weight': 'balanced',\n",
       " 'logreg__dual': False,\n",
       " 'logreg__fit_intercept': True,\n",
       " 'logreg__intercept_scaling': 1,\n",
       " 'logreg__l1_ratio': None,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__multi_class': 'warn',\n",
       " 'logreg__n_jobs': None,\n",
       " 'logreg__penalty': 'l2',\n",
       " 'logreg__random_state': None,\n",
       " 'logreg__solver': 'lbfgs',\n",
       " 'logreg__tol': 0.0001,\n",
       " 'logreg__verbose': 0,\n",
       " 'logreg__warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check all the parameters of the pipeline, do this:\n",
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that we can get parameters out of the pipeline, but we can also set them. Below we set a min_df a little higher which will ignore very infrequent terms, and we set the C parameter in logistic regression a little higher. The higher the C term, the more it tamps down the regularization. \n",
    "\n",
    "We see slightly better results by tweaking a couple of parameters. The main point here, though, was to show how the pipeline made it easy to experiment with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9865702479338843\n",
      "log loss:  0.10179801938057494\n"
     ]
    }
   ],
   "source": [
    "# set a couple of parameters\n",
    "pipe1.set_params(tfidf__min_df=3, logreg__C=2.0).fit(X_train, y_train)\n",
    "pred = pipe1.predict(X_test)\n",
    "print(\"accuracy: \", accuracy_score(y_test, pred))\n",
    "probs = pipe1.predict_proba(X_test)\n",
    "print(\"log loss: \", log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The syntax of sklearn pipelines takes a little getting used to, but does tend to reduce the number of lines of code you have to read through and as we have seen, makes experimenting a little easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
