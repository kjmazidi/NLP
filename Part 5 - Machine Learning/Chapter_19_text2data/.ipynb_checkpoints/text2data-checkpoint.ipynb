{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text to numeric data\n",
    "\n",
    "Machine learning algorithms learn from numeric data. In order for the algorithms to learn from text, the text has to be represented in some numerical form. One way to do this is to represent documents as vectors of frequencies, where each location in the vector represents the count of a specific word in that document. To demonstrate this, a toy corpus is created from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Mary had a little lamb.',\n",
    "         'The lamb followed Mary to school one day.',\n",
    "         'The lamb was white.',\n",
    "         'Mary should not bring a lamb to school.',\n",
    "         'Mary is a little rebel.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "The sklearn CountVectorizer function:\n",
    "* tokenizes the text\n",
    "* assigns a unique integer id to each word in the corpus\n",
    "* counts the frequencies for each word\n",
    "\n",
    "Documentation for CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "The code below first creates a CountVectorizer instance, then fits the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the default options for CountVectorizer. Full descriptions of each parameter are provided in the documentation.\n",
    "\n",
    "Next, corpus counts and names are extracted from the vectorizer fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0]]\n",
      "names: ['bring', 'day', 'followed', 'had', 'is', 'lamb', 'little', 'mary', 'not', 'one', 'rebel', 'school', 'should', 'the', 'to', 'was', 'white']\n"
     ]
    }
   ],
   "source": [
    "corpus_counts = vectorizer.fit_transform(corpus)\n",
    "print(corpus_counts.toarray())\n",
    "print('names:', vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2D array above, each row represents one document in the corpus and each column represents a word. Each element represents the count of that word in that document. As you can see, most counts are zero. These types of vectorized representations of a corpus produce a sparse matrix.\n",
    "\n",
    "### TfidfVectorizer\n",
    "\n",
    "The code below shows how to use the TfidfVectorizer which produces tfidf values instead of frequency counts. Documentation for TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms: {'mary': 7, 'had': 3, 'little': 6, 'lamb': 5, 'the': 13, 'followed': 2, 'to': 14, 'school': 11, 'one': 9, 'day': 1, 'was': 15, 'white': 16, 'should': 12, 'not': 8, 'bring': 0, 'is': 4, 'rebel': 10}\n",
      "\n",
      "idf: [2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 1.18232156\n",
      " 1.69314718 1.18232156 2.09861229 2.09861229 2.09861229 1.69314718\n",
      " 2.09861229 1.69314718 1.69314718 2.09861229 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "# take a look at  the terms and idf\n",
    "print('terms:', vectorizer.vocabulary_)\n",
    "print('\\nidf:', vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating the test data\n",
    "\n",
    "The previous examples showed how to apply the vectorizers to the entire corpus. For supervised machine learning, a better approach is to first divide the data into train/test sets, then fit the vectorizer to the training data only. The test set can be later fit to the vectorizer. In this way, no information from the test set leaks to the training set. The following code demonstrates how to do this on a spam data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv('../data/sms-spam.csv')\n",
    "\n",
    "X = df.text      # features\n",
    "y = df.spam    # targets\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "# use defaults\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorize\n",
    "X_train = vectorizer.fit_transform(X_train) # fit the training data\n",
    "X_test = vectorizer.transform(X_test) # transform only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
